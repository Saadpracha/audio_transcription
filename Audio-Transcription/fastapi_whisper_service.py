import os
import uuid
import json
import time
import logging
import asyncio
import threading
from pathlib import Path
from typing import Optional, Dict, Any
import sys
import subprocess
import shutil
from collections import deque
try:
    import ai_summary  # type: ignore
except Exception:
    ai_summary = None  # type: ignore

from fastapi import FastAPI, BackgroundTasks, HTTPException, Request
from fastapi.responses import HTMLResponse
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates
from datetime import datetime, timezone
from pydantic import BaseModel
import requests
from dotenv import load_dotenv
import re

# Optional parts
import boto3
import openai

# load .env
load_dotenv()

# Basic logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("transcribe_service")

# Try to import user's whisper+diarization class (put your script in whisper_diarizer.py)
try:
    from whisper_diarizer import AudioTranscriberWithDiarization
except Exception:
    # For the simplified flow, we don't require this import. The full flow can use it later.
    AudioTranscriberWithDiarization = None  # type: ignore

# Config
STORAGE = os.getenv("STORAGE", "local")  # 'local' or 's3'
AUDIO_TMP_DIR = Path(os.getenv("AUDIO_TMP_DIR", "./tmp_audio")).resolve()
PERSIST_AUDIO_DIR = Path(os.getenv("AUDIO_DIR", "./audio")).resolve()
OUTPUT_DIR = Path(os.getenv("OUTPUT_DIR", "./output")).resolve()
AUDIO_TMP_DIR.mkdir(parents=True, exist_ok=True)
PERSIST_AUDIO_DIR.mkdir(parents=True, exist_ok=True)
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

S3_BUCKET = os.getenv("S3_BUCKET")
AWS_REGION = os.getenv("AWS_REGION")

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
OPENAI_MODEL = os.getenv("OPENAI_MODEL", "gpt-4o-mini")
if OPENAI_API_KEY:
    openai.api_key = OPENAI_API_KEY

HUGGINGFACE_TOKEN = os.getenv("HUGGINGFACE_TOKEN")

# LeadConnector / HighLevel CRM config
LEADCONNECTOR_BASE_URL = os.getenv("LEADCONNECTOR_BASE_URL", "https://services.leadconnectorhq.com")
# Access token now comes from webhook payload as `token` (Make.com). We intentionally
# do not read a default from environment anymore.


# Whisper config defaults
MODEL_SIZE = os.getenv("WHISPER_MODEL", "small")
DEVICE = os.getenv("DEVICE", "cpu")
COMPUTE_TYPE = os.getenv("COMPUTE_TYPE", "int8")
NUM_THREADS = int(os.getenv("NUM_THREADS", "2"))

PRELOAD_MODELS = os.getenv("PRELOAD_MODELS", "false").lower() in ("1", "true", "yes")

# In-memory job store (replace with DB/Redis for production)
jobs: Dict[str, Dict[str, Any]] = {}

# Job queue management
job_queue = deque()
queue_lock = threading.Lock()
is_processing = False

# Initialize S3 client lazily
_s3_client = None

def get_s3_client():
    global _s3_client
    if _s3_client is None:
        _s3_client = boto3.client(
            "s3",
            aws_access_key_id=os.getenv("AWS_ACCESS_KEY_ID"),
            aws_secret_access_key=os.getenv("AWS_SECRET_ACCESS_KEY"),
            region_name=AWS_REGION,
        )
    return _s3_client


def add_job_to_queue(job_id: str, payload: dict):
    """Add a job to the processing queue."""
    with queue_lock:
        job_queue.append((job_id, payload))
        jobs[job_id]["queue_position"] = len(job_queue)
        logger.info("Job %s added to queue at position %d", job_id, len(job_queue))


def process_queue():
    """Process jobs from the queue one at a time."""
    global is_processing
    
    while True:
        with queue_lock:
            if not job_queue:
                is_processing = False
                break
            job_id, payload = job_queue.popleft()
            is_processing = True
        
        # Update queue positions for remaining jobs
        with queue_lock:
            for i, (queued_job_id, _) in enumerate(job_queue):
                if queued_job_id in jobs:
                    jobs[queued_job_id]["queue_position"] = i + 1
        
        logger.info("Processing job %s from queue", job_id)
        process_job(job_id, payload)
        
        # Small delay before processing next job
        time.sleep(1)


def start_queue_processor():
    """Start the queue processor in a background thread."""
    def queue_worker():
        while True:
            try:
                process_queue()
                time.sleep(5)  # Check for new jobs every 5 seconds
            except Exception as e:
                logger.exception("Error in queue processor: %s", e)
                time.sleep(10)  # Wait longer on error
    
    thread = threading.Thread(target=queue_worker, daemon=True)
    thread.start()
    logger.info("Queue processor started")

# Instantiate a reusable transcriber object (kept for full flow; unused in simple mode)
_transcriber: Optional[AudioTranscriberWithDiarization] = None  # type: ignore

app = FastAPI(title="WhisperX + Diarization Transcription Service")

# Templates and static (mounted lazily if directories exist)
BASE_DIR = Path(__file__).parent.resolve()
TEMPLATES_DIR = BASE_DIR / "templates"
STATIC_DIR = BASE_DIR / "static"
templates = Jinja2Templates(directory=str(TEMPLATES_DIR))

if STATIC_DIR.exists():
    app.mount("/static", StaticFiles(directory=str(STATIC_DIR)), name="static")


class WebhookPayload(BaseModel):
    audio: str
    call_id: Optional[str] = None
    contact_id: Optional[str] = None
    # New fields from Make.com
    token: Optional[str] = None
    make_webhook_url: Optional[str] = None
    client_webhook_url: Optional[str] = None   # optional second webhook
    extra_webhook_urls: Optional[list[str]] = None  # optional list of additional webhooks
    # Google Sheets integration
    googlesheet: Optional[str] = None  # sheet id to echo back to Make.com
    # Tenant/customer routing
    customer_slug: Optional[str] = None  # e.g., "brix" to render /brix/view-session/{id}
    customer_id: Optional[str] = None    # used with CUSTOMER_SLUG_MAP if provided
    account_id: Optional[str] = None     # aliases supported for mapping
    location_id: Optional[str] = None
    caller_name: Optional[str] = None
    contact_first_name: Optional[str] = None
    contact_last_name: Optional[str] = None
    show_prompt: Optional[bool] = False
    language: Optional[str] = "en"
    no_diarization: Optional[bool] = False
    # allow additional fields; Pydantic will ignore unknowns unless configured otherwise


@app.on_event("startup")
def startup_event():
    logger.info("Service starting up...")
    # Start the queue processor
    start_queue_processor()
    # Full flow init is skipped in simple mode


def detect_extension_from_content_type(content_type: str) -> str:
    if not content_type:
        return "mp3"
    content_type = content_type.lower()
    if "mpeg" in content_type or "mp3" in content_type:
        return "mp3"
    if "wav" in content_type:
        return "wav"
    if "mpeg" in content_type:
        return "mp3"
    if "ogg" in content_type or "opus" in content_type:
        return "ogg"
    return "mp3"


def download_audio(url: str, dest_path: Path, auth: Optional[tuple] = None, headers: Optional[dict] = None, timeout: int = 60):
    # Ensure target directory exists
    try:
        dest_path.parent.mkdir(parents=True, exist_ok=True)
    except Exception as mk_ex:
        logger.exception("Failed to ensure tmp dir exists: %s", mk_ex)
    logger.info("Downloading audio from %s to %s", url, dest_path)
    with requests.get(url, stream=True, auth=auth, headers=headers, timeout=timeout) as r:
        r.raise_for_status()
        content_type = r.headers.get("content-type", "")
        # If dest_path doesn't have extension, try to set
        if dest_path.suffix == "":
            ext = detect_extension_from_content_type(content_type)
            dest_path = dest_path.with_suffix("." + ext)
            # Ensure directory remains ensured after suffix change
            dest_path.parent.mkdir(parents=True, exist_ok=True)
        with open(dest_path, "wb") as f:
            for chunk in r.iter_content(chunk_size=8192):
                if chunk:
                    f.write(chunk)
    logger.info("Downloaded audio to %s", dest_path)
    return dest_path


def derive_readable_audio_name(url: str, fallback: str) -> str:
    """Derive a readable filename from the URL path parts, fallback to provided name."""
    try:
        from urllib.parse import urlparse
        parsed = urlparse(url)
        parts = [p for p in parsed.path.split("/") if p]
        if parts:
            # take last 2 parts if available (e.g., Recordings/RE...). Join with '_'.
            tail = parts[-2:]
            name = "_".join(tail)
            # ensure has extension
            if not os.path.splitext(name)[1]:
                name += ".wav"
            return name
    except Exception:
        pass
    return fallback


def upload_to_s3(local_path: Path, s3_key: str) -> str:
    if not S3_BUCKET:
        raise RuntimeError("S3_BUCKET not set in env")
    s3 = get_s3_client()
    logger.info("Uploading %s to s3://%s/%s", local_path, S3_BUCKET, s3_key)
    
    # Determine content type based on file extension
    file_ext = local_path.suffix.lower()
    if file_ext in ['.wav', '.mp3', '.m4a', '.ogg']:
        content_type = 'audio/wav' if file_ext == '.wav' else f'audio/{file_ext[1:]}'
    elif file_ext == '.json':
        content_type = 'application/json'
    else:
        content_type = 'application/octet-stream'
    
    # Upload file with public read access
    s3.upload_file(
        str(local_path), 
        S3_BUCKET, 
        s3_key,
        ExtraArgs={
            'ACL': 'public-read',
            'ContentType': content_type
        }
    )
    
    # Generate direct public URL (not presigned)
    region = AWS_REGION or 'us-east-1'
    if region == 'us-east-1':
        url = f"https://{S3_BUCKET}.s3.amazonaws.com/{s3_key}"
    else:
        url = f"https://{S3_BUCKET}.s3.{region}.amazonaws.com/{s3_key}"
    
    logger.info("Uploaded to S3 with public access: %s", url)
    return url


def send_crm_note(contact_id: str, note_body: str, access_token: str, note_type: str = "general") -> bool:
    """Send a single note to CRM for a contact."""
    url = f"{LEADCONNECTOR_BASE_URL}/contacts/{contact_id}/notes"
    headers = {
        "Authorization": f"Bearer {access_token}",
        "Version": "2021-07-28",
        "Content-Type": "application/json",
    }
    payload_body = {"body": note_body}
    
    try:
        resp = requests.post(url, headers=headers, json=payload_body, timeout=30)
        if resp.ok:
            logger.info("Posted %s note to CRM for contact_id=%s", note_type, contact_id)
            return True
        else:
            logger.warning("CRM %s note post failed: %s %s", note_type, resp.status_code, resp.text)
            return False
    except Exception as crm_ex:
        logger.warning("Failed to POST CRM %s note: %s", note_type, crm_ex)
        return False


def _sanitize_slug(value: str) -> str:
    """Sanitize a slug to contain lowercase letters, digits, and dashes only."""
    value = (value or "").strip().lower()
    value = re.sub(r"[^a-z0-9-]", "-", value)
    value = re.sub(r"-+", "-", value).strip("-")
    return value

def _load_customer_slug_map() -> Dict[str, str]:
    """Load CUSTOMER_SLUG_MAP from env as JSON dict: {"account_id": "slug", ...}."""
    try:
        raw = os.getenv("CUSTOMER_SLUG_MAP", "").strip()
        if not raw:
            return {}
        data = json.loads(raw)
        if isinstance(data, dict):
            # sanitize values
            return {str(k): _sanitize_slug(str(v)) for k, v in data.items()}
    except Exception as e:
        logger.warning("Failed to parse CUSTOMER_SLUG_MAP: %s", e)
    return {}

def resolve_customer_slug_from_payload(payload: Dict[str, Any]) -> Optional[str]:
    """Resolve customer slug using payload fields or CUSTOMER_SLUG_MAP env.

    Priority:
      1) payload.customer_slug or payload.slug (used directly after sanitization)
      2) payload.customer_id/account_id/location_id looked up in CUSTOMER_SLUG_MAP
    """
    try:
        if not isinstance(payload, dict):
            return None
        # Check both customer_slug and slug (alias)
        slug_value = payload.get("customer_slug") or payload.get("slug")
        if slug_value:
            return _sanitize_slug(str(slug_value))
        slug_map = _load_customer_slug_map()
        for key in ("customer_id", "account_id", "location_id"):
            identifier = payload.get(key)
            if identifier is None:
                continue
            identifier_str = str(identifier)
            if identifier_str in slug_map:
                return slug_map[identifier_str]
    except Exception:
        pass
    return None

def send_file_links_note(contact_id: str, job_data: dict, access_token: str) -> bool:
    """Send note with only GUI link."""
    # Get GUI link for the contact/session
    # Use the unique entity_id for file discovery, but contact_id for display
    session_id = job_data.get("entity_id") or contact_id or job_data.get("call_id")
    gui_base = os.getenv("PUBLIC_APP_BASE_URL")  # e.g., https://my-domain.com
    
    # Debug logging
    logger.info(f"Creating GUI link - contact_id: '{contact_id}', session_id: '{session_id}', gui_base: '{gui_base}'")
    
    gui_link = None
    customer_slug = resolve_customer_slug_from_payload(job_data.get("payload", {}))
    if session_id and gui_base:
        ts = int(time.time())
        if customer_slug:
            gui_link = f"{gui_base.rstrip('/')}/{customer_slug}/view-session/{session_id}?t={ts}"
        else:
            gui_link = f"{gui_base.rstrip('/')}/view-session/{session_id}?t={ts}"
        logger.info(f"Generated GUI link: '{gui_link}'")

    if not gui_link:
        logger.warning("No GUI link available to send in file links note")
        return False

    note_body = f"🌐 **View Call Session**\n\n{gui_link}"
    return send_crm_note(contact_id, note_body, access_token, "file_links")

def build_gui_link(session_id: Optional[str], customer_slug: Optional[str] = None) -> Optional[str]:
    """Construct a GUI link for the given session/contact id using PUBLIC_APP_BASE_URL."""
    try:
        gui_base = os.getenv("PUBLIC_APP_BASE_URL")
        if session_id and gui_base:
            ts = int(time.time())
            if customer_slug:
                return f"{gui_base.rstrip('/')}/{customer_slug}/view-session/{session_id}?t={ts}"
            return f"{gui_base.rstrip('/')}/view-session/{session_id}?t={ts}"
    except Exception:
        pass
    return None

def send_make_webhook(job_data: dict, contact_id: Optional[str], call_id: Optional[str], webhook_url: Optional[str] = None, original_payload: Optional[dict] = None, job_id: Optional[str] = None) -> bool:
    """Send the GUI link (and basic metadata) to a Make.com webhook for Sheets/Excel automations.

    The webhook URL is taken from the explicit `webhook_url` argument, or from the
    MAKE_WEBHOOK_URL environment variable if not provided.
    """
    try:
        url = webhook_url or os.getenv("MAKE_WEBHOOK_URL")
        if not url:
            logger.info("No Make.com webhook URL provided; skipping Make webhook post")
            return False

        # Use the unique entity_id for file discovery, but contact_id for display
        session_id = job_data.get("entity_id") or contact_id or call_id
        customer_slug = resolve_customer_slug_from_payload(original_payload or {})
        gui_link = build_gui_link(session_id, customer_slug)
        if not gui_link:
            logger.warning("Could not build GUI link for Make.com webhook; missing session_id or PUBLIC_APP_BASE_URL")
            return False

        # Start with original payload (what Make.com sent us) to echo data back
        payload: Dict[str, Any] = dict(original_payload or {})

        # Add/override our fields
        payload.update({
            "session_id": session_id,
            "entity_id": job_data.get("entity_id"),  # unique per call
            "contact_id": contact_id,
            "call_id": call_id,
            "gui_link": gui_link,
            "created_at": int(time.time()),
            "job_id": job_id,
        })

        # Normalize/echo Google Sheet identifier to help downstream mapping
        try:
            sheet_id = (original_payload or {}).get("googlesheet")
            if sheet_id:
                payload["googlesheet"] = sheet_id
                payload["googlesheet_id"] = sheet_id  # convenience alias
        except Exception:
            pass

        # Include file URLs if available to help downstream automations
        file_fields = {
            "audio_url": job_data.get("audio_s3_url"),
            "transcript_url": job_data.get("transcript_s3_url"),
            "summary_url": job_data.get("summary_s3_url"),
            "prompt_url": job_data.get("prompt_s3_url"),
        }
        payload["files"] = {k: v for k, v in file_fields.items() if v}

        logger.info("Posting GUI link to outbound webhook: %s", url)
        resp = requests.post(url, json=payload, timeout=20)
        if not resp.ok:
            logger.warning("Outbound webhook post failed: %s %s", resp.status_code, resp.text)
            return False
        logger.info("Outbound webhook post succeeded")
        return True
    except Exception as e:
        logger.warning("Failed to send Make.com webhook: %s", e)
        return False

def _public_s3_base_url() -> Optional[str]:
    if not S3_BUCKET:
        return None
    region = AWS_REGION or 'us-east-1'
    return f"https://{S3_BUCKET}.s3.amazonaws.com" if region == 'us-east-1' else f"https://{S3_BUCKET}.s3.{region}.amazonaws.com"

def build_public_s3_url(session_id: str, filename: str, customer_slug: Optional[str] = None) -> Optional[str]:
    base = _public_s3_base_url()
    if not base:
        return None
    if customer_slug:
        return f"{base}/audio/{customer_slug}/{session_id}/{filename}"
    return f"{base}/audio/{session_id}/{filename}"

def build_public_s3_url_from_key(key: str) -> Optional[str]:
    base = _public_s3_base_url()
    if not base:
        return None
    return f"{base}/{key}"

def find_latest_session_files(session_id: str, customer_slug: Optional[str] = None) -> Dict[str, Optional[str]]:
    """Return dict mapping type->S3 key for latest files with timestamped names.
    Keys: summary, transcript, call_to_action, prompt. Values are full S3 keys or None.
    """
    result: Dict[str, Optional[str]] = {"summary": None, "transcript": None, "call_to_action": None, "prompt": None}
    try:
        if not S3_BUCKET:
            return result
        s3 = get_s3_client()
        # Use customer slug in prefix if available: audio/{slug}/{session_id}/
        if customer_slug:
            prefix = f"audio/{customer_slug}/{session_id}/"
        else:
            prefix = f"audio/{session_id}/"
        continuation = None
        contents = []
        while True:
            kwargs = {"Bucket": S3_BUCKET, "Prefix": prefix}
            if continuation:
                kwargs["ContinuationToken"] = continuation
            resp = s3.list_objects_v2(**kwargs)
            contents.extend(resp.get("Contents", []))
            if resp.get("IsTruncated"):
                continuation = resp.get("NextContinuationToken")
            else:
                break

        # Pick latest by suffix
        def pick_latest(suffix: str) -> Optional[str]:
            candidates = [obj for obj in contents if obj.get("Key", "").endswith(suffix)]
            if not candidates:
                return None
            latest = max(candidates, key=lambda o: o.get("LastModified"))
            return latest.get("Key")

        # Audio: pick latest by common audio extensions
        audio_candidates = [obj for obj in contents if obj.get("Key", "").lower().endswith((".wav", ".mp3", ".m4a", ".ogg"))]
        if audio_candidates:
            latest_audio = max(audio_candidates, key=lambda o: o.get("LastModified"))
            result["audio"] = latest_audio.get("Key")
        else:
            result["audio"] = None

        result["transcript"] = pick_latest("_diarized.json")
        result["summary"] = pick_latest("_summary.json")
        result["call_to_action"] = pick_latest("_call_to_action.json")
        result["prompt"] = pick_latest("_prompt.json")
    except Exception as e:
        logger.warning("Failed to list latest session files for %s: %s", session_id, e)
    return result

def fetch_json(url: str) -> Optional[dict]:
    try:
        resp = requests.get(url, timeout=20)
        if resp.ok:
            return resp.json()
    except Exception as e:
        logger.warning("Failed to fetch JSON from %s: %s", url, e)
    return None

@app.get("/view-session/{session_id}", response_class=HTMLResponse)
def view_session(request: Request, session_id: str):
    """Render a simple Jinja2 dashboard for a session/contact id.
    It attempts to load summary, transcript (diarized), call-to-action, and prompt files.
    """
    # We try to locate latest files by prefix. If exact names not known, we attempt common suffixes.
    # Expected naming: {session_id}_{timestamp}_summary.json, ..._call_to_action.json, ..._diarized.json
    # For now, we probe by trying to get the latest by hitting a small list of plausible names.
    # If you maintain exact filenames, consider storing them or indexing. Here we allow 404s gracefully.

    # Try to discover latest by probing a small time window (optional). Simpler: if CRM only passes session link,
    # we can attempt to fetch a listing via S3 ListObjectsV2, but bucket is public; List requires creds typically.
    # We'll instead try a heuristic: request a diarized listing via a known example name from job data isn't available here.

    # Heuristic: try up to N recent timestamps embedded in typical patterns is complex; keep simple: attempt to fetch
    # a manifest if exists, else try the 'most recent' patterns are unknown. We will instead attempt with wildcards not possible over HTTP.
    # Therefore, document expected names and rely on that.

    # Prefer latest timestamped files discovered via S3 listing
    latest_keys = find_latest_session_files(session_id)
    data = {"summary": None, "transcript": None, "call_to_action": None, "prompt": None}
    urls: Dict[str, Optional[str]] = {"summary": None, "transcript": None, "prompt": None, "audio": None}

    # For each type, if a key was found use exact URL; otherwise fall back to non-timestamp name
    fallbacks = {
        "summary": f"{session_id}_summary.json",
        "transcript": f"{session_id}_diarized.json",
        "prompt": f"{session_id}_prompt.json",
    }

    for key in ["summary", "transcript", "prompt"]:
        if latest_keys.get(key):
            url = build_public_s3_url_from_key(latest_keys[key])  # preserves timestamp
        else:
            url = build_public_s3_url(session_id, fallbacks[key])
        urls[key] = url
        if url:
            data[key] = fetch_json(url)

    # Audio url if found
    if latest_keys.get("audio"):
        urls["audio"] = build_public_s3_url_from_key(latest_keys["audio"]) 

    # Extract call to action data from summary file
    if data.get("summary") and isinstance(data["summary"], dict):
        call_to_action_data = (
            data["summary"].get("call_to_action")
            or data["summary"].get("call_to_action_items")
        )
        if call_to_action_data:
            data["call_to_action"] = {"call_to_action": call_to_action_data}

    # Compute human-readable UTC for prompt created_at if available
    if data.get("prompt") and isinstance(data.get("prompt"), dict):
        try:
            created_raw = data["prompt"].get("created_at")
            if isinstance(created_raw, (int, float)):
                dt = datetime.fromtimestamp(float(created_raw), tz=timezone.utc)
                data["prompt"]["created_at_utc"] = dt.strftime("%Y-%m-%d %H:%M UTC")
        except Exception as _e:
            pass

    return templates.TemplateResponse(
        "session_view.html",
        {
            "request": request,
            "session_id": session_id,
            "summary": data["summary"],
            "transcript": data["transcript"],
            "call_to_action": data["call_to_action"],
            "prompt": data["prompt"],
            "urls": urls,
        },
    )


# Support tenant-prefixed GUI links like /{customer_slug}/view-session/{session_id}
# This simply forwards to the same renderer while accepting and ignoring the slug for routing.
@app.get("/{customer_slug}/view-session/{session_id}", response_class=HTMLResponse)
def view_session_with_slug(request: Request, customer_slug: str, session_id: str):
    """Render session view with customer slug context for S3 file discovery."""
    # Prefer latest timestamped files discovered via S3 listing with slug context
    latest_keys = find_latest_session_files(session_id, customer_slug)
    data = {"summary": None, "transcript": None, "call_to_action": None, "prompt": None}
    urls: Dict[str, Optional[str]] = {"summary": None, "transcript": None, "prompt": None, "audio": None}

    # For each type, if a key was found use exact URL; otherwise fall back to non-timestamp name
    fallbacks = {
        "summary": f"{session_id}_summary.json",
        "transcript": f"{session_id}_diarized.json",
        "prompt": f"{session_id}_prompt.json",
    }

    for key in ["summary", "transcript", "prompt"]:
        if latest_keys.get(key):
            url = build_public_s3_url_from_key(latest_keys[key])  # preserves timestamp
        else:
            # Build URL with slug context for fallback files
            if customer_slug:
                url = build_public_s3_url(session_id, fallbacks[key], customer_slug)
            else:
                url = build_public_s3_url(session_id, fallbacks[key])
        urls[key] = url
        if url:
            data[key] = fetch_json(url)

    # Audio url if found
    if latest_keys.get("audio"):
        urls["audio"] = build_public_s3_url_from_key(latest_keys["audio"]) 

    # Extract call to action data from summary file
    if data.get("summary") and isinstance(data["summary"], dict):
        call_to_action_data = (
            data["summary"].get("call_to_action")
            or data["summary"].get("call_to_action_items")
        )
        if call_to_action_data:
            data["call_to_action"] = {"call_to_action": call_to_action_data}

    # Compute human-readable UTC for prompt created_at if available
    if data.get("prompt") and isinstance(data.get("prompt"), dict):
        try:
            created_raw = data["prompt"].get("created_at")
            if isinstance(created_raw, (int, float)):
                dt = datetime.fromtimestamp(float(created_raw), tz=timezone.utc)
                data["prompt"]["created_at_utc"] = dt.strftime("%Y-%m-%d %H:%M UTC")
        except Exception as _e:
            pass

    return templates.TemplateResponse(
        "session_view.html",
        {
            "request": request,
            "session_id": session_id,
            "summary": data["summary"],
            "transcript": data["transcript"],
            "call_to_action": data["call_to_action"],
            "prompt": data["prompt"],
            "urls": urls,
        },
    )


def send_summary_note(contact_id: str, job_data: dict, access_token: str) -> bool:
    """Send second note with summary."""
    summary_s3_url = job_data.get("summary_s3_url")
    if not summary_s3_url:
        logger.warning("No summary file available to send")
        return False
    
    try:
        # Download and read summary from S3
        summary_resp = requests.get(summary_s3_url, timeout=30)
        if not summary_resp.ok:
            logger.warning("Failed to download summary from S3: %s", summary_resp.status_code)
            return False
        
        summary_data = summary_resp.json()
        summary_text = summary_data.get("summary", "No summary available")
        
        note_body = f"📋 **Call Summary**\n\n{summary_text}"
        return send_crm_note(contact_id, note_body, access_token, "summary")
        
    except Exception as e:
        logger.warning("Failed to process summary for CRM note: %s", e)
        return False


def send_call_to_action_note(contact_id: str, job_data: dict, access_token: str) -> bool:
    """Send third note with call to action items."""
    call_to_action_s3_url = job_data.get("call_to_action_s3_url")
    if not call_to_action_s3_url:
        logger.warning("No call to action file available to send")
        return False
    
    try:
        # Download and read call to action from S3
        cta_resp = requests.get(call_to_action_s3_url, timeout=30)
        if not cta_resp.ok:
            logger.warning("Failed to download call to action from S3: %s", cta_resp.status_code)
            return False
        
        cta_data = cta_resp.json()
        call_to_action_items = cta_data.get("call_to_action", [])
        
        if not call_to_action_items:
            logger.warning("No call to action items found in file")
            return False
        
        # Format call to action items
        cta_lines = ["✅ **Call to Action Items**\n"]
        for i, item in enumerate(call_to_action_items, 1):
            action = item.get("item", "No action specified")
            owner = item.get("owner", "Unassigned")
            due = item.get("due", "No due date")
            
            cta_lines.append(f"{i}. **{action}**")
            cta_lines.append(f"   👤 Owner: {owner}")
            if due and due != "":
                cta_lines.append(f"   📅 Due: {due}")
            cta_lines.append("")  # Empty line for spacing
        
        note_body = "\n".join(cta_lines)
        return send_crm_note(contact_id, note_body, access_token, "call_to_action")
        
    except Exception as e:
        logger.warning("Failed to process call to action for CRM note: %s", e)
        return False


def create_prompt_file(prompt_path: Path, caller_name: str = "", contact_first_name: str = "", contact_last_name: str = ""):
    """Create a prompt.json file with the current prompt template and caller information."""
    try:
        # Get the default prompt from ai_summary
        if ai_summary and hasattr(ai_summary, "get_default_prompt"):
            prompt_template = ai_summary.get_default_prompt()
        else:
            # Fallback prompt if ai_summary is not available
            prompt_template = """You are an expert meeting and call analysis assistant.
Your job is to read the transcript of a sales or support call and return ONLY valid JSON.

The JSON must follow this schema exactly:
{{
  "summary": "string – a clear, concise summary of the call (cover goals, decisions, commitments, numbers, and outcomes).",
  "call_to_action_items": [
    {{
      "item": "string – the specific task or next step",
      "owner": "string – person responsible (use speaker names if clear, else leave empty)",
      "due": "string – due date if mentioned, else empty"
    }}
  ],
  "call_quality_feedback": {{
    "strengths": ["list of things done well – rapport, clarity, active listening, etc."],
    "improvements": ["list of ways caller/agent can improve – tone, pacing, missing info, objection handling, etc."]
  }}
}}

Guidelines:
- Be **brief but insightful**: a short call = short summary; a long/complex call = more detail.
- Summaries should highlight outcomes, decisions, and commitments – not just a play-by-play.
- Call-to-actions must be **specific and actionable**. Avoid vague items like "follow up" unless no detail is provided.
- Owners: infer from speaker labels where possible. Example: if "Agent" says "I will send the proposal", set owner = "Agent".
- If dates are mentioned, capture them (e.g. "by Friday"); if not, leave due = "".
- Feedback should be constructive: balance what went well with what could improve.

TRANSCRIPT:
<<<
{{full_transcript}}
>>>"""

        # Create prompt data with caller and contact information
        prompt_data = {
            "prompt": prompt_template,
            "caller_information": {
                "full_name": caller_name.strip()
            },
            "contact_information": {
                "first_name": contact_first_name,
                "last_name": contact_last_name,
                "full_name": f"{contact_first_name} {contact_last_name}".strip()
            },
            "created_at": time.time(),
            "version": "1.0"
        }
        
        # Write to file
        with open(prompt_path, 'w', encoding='utf-8') as f:
            json.dump(prompt_data, f, indent=4, ensure_ascii=False)
        
        logger.info("Created prompt file: %s", prompt_path)
        return True
        
    except Exception as e:
        logger.exception("Failed to create prompt file: %s", e)
        return False


def send_crm_notes(contact_id: str, job_data: dict, access_token: str):
    """Send only the GUI link note to CRM."""
    logger.info("Sending GUI link note to CRM for contact_id=%s", contact_id)

    # Track success of file link note only
    notes_sent = {
        "file_links": False
    }

    # Send only file links note
    notes_sent["file_links"] = send_file_links_note(contact_id, job_data, access_token)

    # Log results
    if notes_sent["file_links"]:
        logger.info("Successfully sent notes to CRM: file_links")
    else:
        logger.warning("Failed to send notes to CRM: file_links")

    # Store results in job data
    job_data["crm_notes_sent"] = notes_sent


def summarize_transcript_with_openai(transcription: dict) -> str:
    if not OPENAI_API_KEY:
        raise RuntimeError("OPENAI_API_KEY is not set")

    # Build a readable transcript (speaker + text + time)
    lines = []
    for seg in transcription.get("segments", []):
        start = seg.get("start")
        speaker = seg.get("speaker", "Unknown")
        text = seg.get("text", "").strip()
        lines.append(f"[{start:.2f}] {speaker}: {text}")

    # join; if extremely long, we could chunk - for simplicity we send everything (you may want to chunk in production)
    joined = "\n".join(lines)

    system_prompt = (
        "You are an assistant that creates concise call summaries. Given the transcript below, output:\n"
        "1) A short TL;DR (1-2 lines)\n2) Key points / decisions (bullet list)\n3) Action items with owners (if mentioned)\n4) Important quotes or timestamps\n        "
    )

    user_prompt = "Transcript:\n\n" + joined

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt},
    ]

    logger.info("Sending transcript to OpenAI for summarization (model=%s)", OPENAI_MODEL)
    resp = openai.ChatCompletion.create(
        model=OPENAI_MODEL,
        messages=messages,
        temperature=0.0,
        max_tokens=1500,
    )
    summary_text = resp["choices"][0]["message"]["content"].strip()
    return summary_text


def process_job(job_id: str, payload: dict):
    """Simplified pipeline: download audio, run CLI transcriber, upload to S3."""
    logger.info("Job %s started (simple mode)", job_id)
    jobs[job_id] = {"status": "started", "created_at": time.time(), "payload": payload}

    local_path = None
    run_output_dir = None
    transcript_path = None

    try:
        audio_url = payload.get("audio")
        if not audio_url:
            raise ValueError("payload must include 'audio' url")
        # Prefer contact_id over call_id; maintain call_id for backward compatibility
        contact_id = payload.get("contact_id")
        call_id = payload.get("call_id")
        # Create unique session per call: contact_id + call_id + timestamp
        # This ensures each call gets its own files even for the same contact
        timestamp = int(time.time())
        if contact_id and call_id:
            entity_id = f"{contact_id}_{call_id}_{timestamp}"
        elif contact_id:
            entity_id = f"{contact_id}_{timestamp}"
        elif call_id:
            entity_id = f"{call_id}_{timestamp}"
        else:
            entity_id = f"session_{timestamp}_{uuid.uuid4().hex[:8]}"

        jobs[job_id]["status"] = "downloading"
        # Resolve and store customer slug early for downstream uses
        try:
            jobs[job_id]["customer_slug"] = resolve_customer_slug_from_payload(payload)  # optional
        except Exception:
            pass
        # build a readable persisted audio name and a temp download target
        # Add timestamp to ensure unique file names
        readable_name = derive_readable_audio_name(audio_url, f"{entity_id}_{uuid.uuid4().hex[:8]}.wav")
        temp_download_name = f"{entity_id}_{uuid.uuid4().hex[:8]}"
        local_path = AUDIO_TMP_DIR / temp_download_name
        # support basic auth via audio_auth or headers
        audio_auth = None
        if isinstance(payload.get("audio_auth"), dict):
            aa = payload.get("audio_auth")
            audio_auth = (aa.get("username"), aa.get("password"))
        headers = payload.get("audio_headers")
        local_path = download_audio(audio_url, local_path, auth=audio_auth, headers=headers)

        # Run your CLI transcriber script
        jobs[job_id]["status"] = "transcribing"
        # Create temporary local output directory for processing
        run_dir_name = Path(readable_name).stem
        run_output_dir = OUTPUT_DIR / run_dir_name
        run_output_dir.mkdir(parents=True, exist_ok=True)

        transcript_filename = f"{entity_id}_diarized.json"
        transcript_path = run_output_dir / transcript_filename
        script_path = (Path(__file__).parent / "audio_transcribe_diarization.py").resolve()
        # Determine CLI model/device/threads from environment; normalize model names to CLI choices
        model_env = os.getenv("WHISPER_MODEL", MODEL_SIZE)
        cli_model = (model_env or "small").lower()
        if cli_model.startswith("large"):
            cli_model = "large"
        elif cli_model not in {"tiny", "base", "small", "medium", "large"}:
            cli_model = "small"

        cli_threads = str(NUM_THREADS)
        cli_device = DEVICE  # CLI currently supports only "cpu"

        cmd = [
            sys.executable,
            str(script_path),
            str(local_path),
            "-o", str(transcript_path),
            "-l", payload.get("language", "en"),
            "-m", cli_model,
            "-t", cli_threads,
            "-d", cli_device,
        ]
        if payload.get("no_diarization"):
            cmd.append("--no-diarization")
        logger.info("Running transcriber: %s", " ".join(cmd))
        subprocess.run(cmd, check=True, cwd=str(Path(__file__).parent))

        # Summarize using ai_summary.process_transcript if available
        summary_path = None
        call_to_action_path = None
        prompt_path = None
        try:
            if ai_summary is not None and hasattr(ai_summary, "process_transcript"):
                jobs[job_id]["status"] = "summarizing"
                # Use a proper base output file so ai_summary writes:
                # <base>.json, <base>_summary.json, <base>_call_to_action.json
                base_output_filename = f"{entity_id}.json"
                base_output_path = run_output_dir / base_output_filename
                
                # Get caller and contact information from payload
                caller_name = payload.get("caller_name") or ""
                # For ai_summary.process_transcript we still split into first/last locally
                split_first = ""
                split_last = ""
                try:
                    parts = [p for p in caller_name.strip().split(" ") if p]
                    if len(parts) == 1:
                        split_first = parts[0]
                    elif len(parts) >= 2:
                        split_first = parts[0]
                        split_last = " ".join(parts[1:])
                except Exception:
                    pass
                contact_first_name = payload.get("contact_first_name") or ""
                contact_last_name = payload.get("contact_last_name") or ""
                
                # Process transcript with caller information
                ai_summary.process_transcript(
                    str(transcript_path), 
                    str(base_output_path), 
                    OPENAI_API_KEY,
                    "prompt.json",  # prompt file path
                    split_first,
                    split_last
                )
                
                # Expected generated files
                summary_path = run_output_dir / f"{entity_id}_summary.json"
                
                # Call to action data is now included in summary file
                call_to_action_path = None
                    
                # Create prompt.json file if show_prompt is true
                if payload.get("show_prompt", False):
                    prompt_filename = f"{entity_id}_prompt.json"
                    prompt_path = run_output_dir / prompt_filename
                    create_prompt_file(prompt_path, caller_name, contact_first_name, contact_last_name)
                    
            else:
                logger.warning("ai_summary.process_transcript not available; skipping summarization")
        except Exception as summarize_ex:
            logger.exception("Summarization step failed: %s", summarize_ex)

        # Inject caller/contact metadata into summary JSON (without changing summary text)
        try:
            if summary_path and summary_path.exists():
                with open(summary_path, "r", encoding="utf-8") as sf:
                    summary_json = json.load(sf)
                # Prepare names again (safe if already set above; handle path where ai_summary block skipped)
                caller_name = payload.get("caller_name") or ""
                contact_first_name = payload.get("contact_first_name") or ""
                contact_last_name = payload.get("contact_last_name") or ""

                summary_json.setdefault("metadata", {})
                summary_json["metadata"].update({
                    "caller": {
                        "full_name": caller_name.strip()
                    },
                    "contact": {
                        "first_name": contact_first_name,
                        "last_name": contact_last_name,
                        "full_name": f"{contact_first_name} {contact_last_name}".strip()
                    }
                })
                with open(summary_path, "w", encoding="utf-8") as sf:
                    json.dump(summary_json, sf, indent=2, ensure_ascii=False)
        except Exception as meta_ex:
            logger.warning("Failed to inject metadata into summary file: %s", meta_ex)

        # Upload files to S3
        jobs[job_id]["status"] = "uploading_to_s3"
        # Use customer slug in S3 path if available: audio/{slug}/{contact_id}/
        customer_slug = jobs[job_id].get("customer_slug")
        if customer_slug:
            s3_base_key = f"audio/{customer_slug}/{entity_id}/"
        else:
            s3_base_key = f"audio/{entity_id}/"
        
        # Upload audio file
        audio_s3_key = f"{s3_base_key}{readable_name}"
        audio_s3_url = upload_to_s3(local_path, audio_s3_key)
        jobs[job_id]["audio_s3_url"] = audio_s3_url
        jobs[job_id]["audio_s3_key"] = audio_s3_key

        # Upload transcription file
        transcript_s3_key = f"{s3_base_key}{transcript_filename}"
        transcript_s3_url = upload_to_s3(transcript_path, transcript_s3_key)
        jobs[job_id]["transcript_s3_url"] = transcript_s3_url
        jobs[job_id]["transcript_s3_key"] = transcript_s3_key

        # Upload summary file if it exists
        if summary_path and summary_path.exists():
            summary_s3_key = f"{s3_base_key}{summary_path.name}"
            summary_s3_url = upload_to_s3(summary_path, summary_s3_key)
            jobs[job_id]["summary_s3_url"] = summary_s3_url
            jobs[job_id]["summary_s3_key"] = summary_s3_key

        # Call to action data is now included in summary file, no separate upload needed

        # Upload prompt file if it exists (when show_prompt=true)
        if prompt_path and prompt_path.exists():
            prompt_s3_key = f"{s3_base_key}{prompt_path.name}"
            prompt_s3_url = upload_to_s3(prompt_path, prompt_s3_key)
            jobs[job_id]["prompt_s3_url"] = prompt_s3_url
            jobs[job_id]["prompt_s3_key"] = prompt_s3_key

        # Send GUI link to Make.com webhook if configured
        try:
            # Collect all outbound webhooks: env default, make.com, client-specific, and extras
            webhook_urls = []
            env_url = os.getenv("MAKE_WEBHOOK_URL")
            if env_url:
                webhook_urls.append(env_url)
            if payload.get("make_webhook_url"):
                webhook_urls.append(payload.get("make_webhook_url"))
            if payload.get("client_webhook_url"):
                webhook_urls.append(payload.get("client_webhook_url"))
            if isinstance(payload.get("extra_webhook_urls"), list):
                webhook_urls.extend([u for u in payload.get("extra_webhook_urls") if isinstance(u, str) and u])

            # De-duplicate while preserving order
            seen = set()
            unique_urls = []
            for u in webhook_urls:
                if u not in seen:
                    seen.add(u)
                    unique_urls.append(u)

            for url in unique_urls:
                send_make_webhook(jobs[job_id], contact_id, call_id, url, original_payload=payload, job_id=job_id)
        except Exception as make_ex:
            logger.warning("Unexpected error while posting to Make.com webhook: %s", make_ex)

        # Send GUI link note to CRM using contact_id and token from payload (if provided)
        try:
            access_token = payload.get("token")
            if contact_id and access_token:
                jobs[job_id]["status"] = "posting_crm_notes"
                
                # Send only file links note
                send_crm_notes(contact_id, jobs[job_id], access_token)
                
            elif contact_id and not access_token:
                logger.warning("No access token provided in payload; skipping CRM note post")
            else:
                logger.info("No contact_id provided; skipping CRM note post")
        except Exception as post_ex:
            logger.warning("Unexpected error while posting CRM notes: %s", post_ex)

        jobs[job_id]["status"] = "done"
        jobs[job_id]["finished_at"] = time.time()
        jobs[job_id]["s3_base_key"] = s3_base_key
        jobs[job_id]["entity_id"] = entity_id  # unique per call
        jobs[job_id]["contact_id"] = contact_id  # original contact for display
        jobs[job_id]["call_id"] = call_id

        logger.info("Job %s finished successfully (simple mode)", job_id)

    except Exception as e:
        logger.exception("Job %s failed: %s", job_id, e)
        jobs[job_id]["status"] = "error"
        jobs[job_id]["error"] = str(e)
    finally:
        # Clean up local temporary files and working directory regardless of outcome
        try:
            if local_path and isinstance(local_path, Path) and local_path.exists():
                local_path.unlink()
            if run_output_dir and isinstance(run_output_dir, Path) and run_output_dir.exists():
                shutil.rmtree(run_output_dir)
            logger.info("Cleaned up local temporary files for job %s", job_id)
        except Exception as cleanup_ex:
            logger.warning("Failed to clean up local files for job %s: %s", job_id, cleanup_ex)


@app.post("/webhook")
def webhook_listener(payload: dict):
    """Receives webhook (from Make or direct CRM). Payload must contain `audio` url. Optionally call_id.

    Example payload:
    {
      "audio": "https://api.twilio.com/.../Recordings/RExxx",
      "contact_id": "abc123",
      "make_webhook_url": "https://hook.integromat.com/xxxxx",  # optional; else uses MAKE_WEBHOOK_URL env
      "customer_slug": "brix",                                   # optional; results in /brix/view-session/{id}
      "customer_id": "cust_001",                                 # optional; resolved via CUSTOMER_SLUG_MAP
      "caller_first_name": "John",
      "caller_last_name": "Doe",
      "show_prompt": true,
      "language": "en",
      "call_id": "12345"  # optional/backward compatibility
    }
    
    Returns:
    {
      "job_id": "uuid",
      "status_url": "/jobs/{job_id}",
      "contact_id": "abc123",
      "call_id": "12345",
      "status": "queued",
      "queue_position": 1
    }
    """
    logger.info("Received webhook payload: keys=%s", list(payload.keys()))
    job_id = str(uuid.uuid4())
    contact_id = payload.get("contact_id")
    call_id = payload.get("call_id")
    entity_id = contact_id or call_id or str(uuid.uuid4())
    
    # Initialize job status
    jobs[job_id] = {
        "status": "queued", 
        "created_at": time.time(), 
        "contact_id": contact_id,
        "call_id": call_id,
        "queue_position": 0
    }
    
    # Add to queue
    add_job_to_queue(job_id, payload)
    
    # Get current queue position
    current_position = jobs[job_id].get("queue_position", 0)
    
    return {
        "job_id": job_id, 
        "status_url": f"/jobs/{job_id}",
        "contact_id": contact_id,
        "call_id": call_id,
        "status": "queued",
        "queue_position": current_position
    }


@app.get("/jobs/{job_id}")
def get_job(job_id: str):
    """Get job status and results. When status is 'done', includes all S3 links.
    
    Response format when completed:
    {
      "job_id": "uuid",
      "call_id": "12345",
      "status": "done",
      "created_at": 1234567890,
      "finished_at": 1234567891,
      "s3_base_key": "audio/12345/",
      "files": {
        "audio": {
          "url": "https://s3.amazonaws.com/bucket/audio/12345/file.wav",
          "key": "audio/12345/file.wav",
          "filename": "12345_1234567890_abc12345.wav"
        },
        "transcription": {
          "url": "https://s3.amazonaws.com/bucket/audio/12345/transcript.json",
          "key": "audio/12345/12345_1234567890_diarized.json",
          "filename": "12345_1234567890_diarized.json"
        },
        "summary": {
          "url": "https://s3.amazonaws.com/bucket/audio/12345/summary.json",
          "key": "audio/12345/12345_1234567890_summary.json",
          "filename": "12345_1234567890_summary.json"
        },
        "call_to_action": {
          "url": "https://s3.amazonaws.com/bucket/audio/12345/call_to_action.json",
          "key": "audio/12345/12345_1234567890_call_to_action.json",
          "filename": "12345_1234567890_call_to_action.json"
        }
      }
    }
    """
    job = jobs.get(job_id)
    if not job:
        raise HTTPException(status_code=404, detail="job_id not found")
    
    # If job is completed, structure the response with all S3 links
    if job.get("status") == "done":
        response = {
            "job_id": job_id,
            "contact_id": job.get("contact_id"),
            "call_id": job.get("call_id"),
            "status": job.get("status"),
            "created_at": job.get("created_at"),
            "finished_at": job.get("finished_at"),
            "s3_base_key": job.get("s3_base_key"),
            "files": {}
        }
        
        # Add audio file info
        if job.get("audio_s3_url"):
            response["files"]["audio"] = {
                "url": job.get("audio_s3_url"),
                "key": job.get("audio_s3_key"),
                "filename": job.get("audio_s3_key", "").split("/")[-1] if job.get("audio_s3_key") else None
            }
        
        # Add transcription file info
        if job.get("transcript_s3_url"):
            response["files"]["transcription"] = {
                "url": job.get("transcript_s3_url"),
                "key": job.get("transcript_s3_key"),
                "filename": job.get("transcript_s3_key", "").split("/")[-1] if job.get("transcript_s3_key") else None
            }
        
        # Add summary file info (if exists)
        if job.get("summary_s3_url"):
            response["files"]["summary"] = {
                "url": job.get("summary_s3_url"),
                "key": job.get("summary_s3_key"),
                "filename": job.get("summary_s3_key", "").split("/")[-1] if job.get("summary_s3_key") else None
            }
        
        # Add call-to-action file info (if exists)
        if job.get("call_to_action_s3_url"):
            response["files"]["call_to_action"] = {
                "url": job.get("call_to_action_s3_url"),
                "key": job.get("call_to_action_s3_key"),
                "filename": job.get("call_to_action_s3_key", "").split("/")[-1] if job.get("call_to_action_s3_key") else None
            }
        
        # Add prompt file info (if exists)
        if job.get("prompt_s3_url"):
            response["files"]["prompt"] = {
                "url": job.get("prompt_s3_url"),
                "key": job.get("prompt_s3_key"),
                "filename": job.get("prompt_s3_key", "").split("/")[-1] if job.get("prompt_s3_key") else None
            }
        
        return response
    
    # For non-completed jobs, return the basic job info with queue status
    response = dict(job)
    
    # Add queue information
    with queue_lock:
        response["is_processing"] = is_processing
        response["queue_length"] = len(job_queue)
        if "queue_position" in job:
            response["queue_position"] = job["queue_position"]
    
    return response


@app.get("/queue/status")
def get_queue_status():
    """Get current queue status and statistics."""
    with queue_lock:
        return {
            "is_processing": is_processing,
            "queue_length": len(job_queue),
            "total_jobs": len(jobs),
            "completed_jobs": len([j for j in jobs.values() if j.get("status") == "done"]),
            "failed_jobs": len([j for j in jobs.values() if j.get("status") == "error"]),
            "queued_jobs": len([j for j in jobs.values() if j.get("status") == "queued"])
        }


@app.get("/health")
def health():
    return {"status": "ok", "models_preloaded": PRELOAD_MODELS}


if __name__ == "__main__":
    import uvicorn
    uvicorn.run("fastapi_whisper_service:app", host="0.0.0.0", port=int(os.getenv("PORT", 8088)), log_level="info")
